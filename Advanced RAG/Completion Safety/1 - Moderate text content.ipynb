{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moderate text content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Copyright (c) Microsoft. All rights reserved.\n",
    "# To learn more, please visit the documentation - Quickstart: Azure Content Safety: https://aka.ms/acsstudiodoc\n",
    "#\n",
    "import enum\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "#create a .env file with the following variables and replace with your values\n",
    "AI_STUDIO_CONNECTION_ENDPOINT = os.getenv(\"AI_STUDIO_CONNECTION_ENDPOINT\")\n",
    "AI_STUDIO_CONNECTION_KEY = os.getenv(\"AI_STUDIO_CONNECTION_KEY\")\n",
    "GPT4o_API_KEY = os.getenv(\"GPT4o_API_KEY\")\n",
    "GPT4o_DEPLOYMENT_ENDPOINT = os.getenv(\"GPT4o_DEPLOYMENT_ENDPOINT\")\n",
    "GPT4o_DEPLOYMENT_NAME = os.getenv(\"GPT4o_DEPLOYMENT_NAME\")\n",
    "\n",
    "# Replace with your own subscription_key and endpoint\n",
    "subscription_key = AI_STUDIO_CONNECTION_KEY\n",
    "endpoint = AI_STUDIO_CONNECTION_ENDPOINT\n",
    "api_version = \"2024-02-15-preview\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum\n",
    "import json\n",
    "import requests\n",
    "from typing import Union\n",
    "\n",
    "\n",
    "class MediaType(enum.Enum):\n",
    "    Text = 1\n",
    "    Image = 2\n",
    "\n",
    "\n",
    "class Category(enum.Enum):\n",
    "    Hate = 1\n",
    "    SelfHarm = 2\n",
    "    Sexual = 3\n",
    "    Violence = 4\n",
    "\n",
    "\n",
    "class Action(enum.Enum):\n",
    "    Accept = 1\n",
    "    Reject = 2\n",
    "\n",
    "\n",
    "class DetectionError(Exception):\n",
    "    def __init__(self, code: str, message: str) -> None:\n",
    "        \"\"\"\n",
    "        Exception raised when there is an error in detecting the content.\n",
    "\n",
    "        Args:\n",
    "        - code (str): The error code.\n",
    "        - message (str): The error message.\n",
    "        \"\"\"\n",
    "        self.code = code\n",
    "        self.message = message\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"DetectionError(code={self.code}, message={self.message})\"\n",
    "\n",
    "\n",
    "class Decision(object):\n",
    "    def __init__(\n",
    "        self, suggested_action: Action, action_by_category: dict[Category, Action]\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Represents the decision made by the content moderation system.\n",
    "\n",
    "        Args:\n",
    "        - suggested_action (Action): The suggested action to take.\n",
    "        - action_by_category (dict[Category, Action]): The action to take for each category.\n",
    "        \"\"\"\n",
    "        self.suggested_action = suggested_action\n",
    "        self.action_by_category = action_by_category\n",
    "\n",
    "\n",
    "class ContentSafety(object):\n",
    "    def __init__(self, endpoint: str, subscription_key: str, api_version: str) -> None:\n",
    "        \"\"\"\n",
    "        Creates a new ContentSafety instance.\n",
    "\n",
    "        Args:\n",
    "        - endpoint (str): The endpoint URL for the Content Safety API.\n",
    "        - subscription_key (str): The subscription key for the Content Safety API.\n",
    "        - api_version (str): The version of the Content Safety API to use.\n",
    "        \"\"\"\n",
    "        self.endpoint = endpoint\n",
    "        self.subscription_key = subscription_key\n",
    "        self.api_version = api_version\n",
    "\n",
    "    def build_url(self, media_type: MediaType) -> str:\n",
    "        \"\"\"\n",
    "        Builds the URL for the Content Safety API based on the media type.\n",
    "\n",
    "        Args:\n",
    "        - media_type (MediaType): The type of media to analyze.\n",
    "\n",
    "        Returns:\n",
    "        - str: The URL for the Content Safety API.\n",
    "        \"\"\"\n",
    "        if media_type == MediaType.Text:\n",
    "            return f\"{self.endpoint}/contentsafety/text:analyze?api-version={self.api_version}\"\n",
    "        elif media_type == MediaType.Image:\n",
    "            return f\"{self.endpoint}/contentsafety/image:analyze?api-version={self.api_version}\"\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid Media Type {media_type}\")\n",
    "\n",
    "    def build_headers(self) -> dict[str, str]:\n",
    "        \"\"\"\n",
    "        Builds the headers for the Content Safety API request.\n",
    "\n",
    "        Returns:\n",
    "        - dict[str, str]: The headers for the Content Safety API request.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"Ocp-Apim-Subscription-Key\": self.subscription_key,\n",
    "            \"Content-Type\": \"application/json\",\n",
    "        }\n",
    "\n",
    "    def build_request_body(\n",
    "        self,\n",
    "        media_type: MediaType,\n",
    "        content: str,\n",
    "        blocklists: list[str],\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Builds the request body for the Content Safety API request.\n",
    "\n",
    "        Args:\n",
    "        - media_type (MediaType): The type of media to analyze.\n",
    "        - content (str): The content to analyze.\n",
    "        - blocklists (list[str]): The blocklists to use for text analysis.\n",
    "\n",
    "        Returns:\n",
    "        - dict: The request body for the Content Safety API request.\n",
    "        \"\"\"\n",
    "        if media_type == MediaType.Text:\n",
    "            return {\n",
    "                \"text\": content,\n",
    "                \"blocklistNames\": blocklists,\n",
    "            }\n",
    "        elif media_type == MediaType.Image:\n",
    "            return {\"image\": {\"content\": content}}\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid Media Type {media_type}\")\n",
    "\n",
    "    def detect(\n",
    "        self,\n",
    "        media_type: MediaType,\n",
    "        content: str,\n",
    "        blocklists: list[str] = [],\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Detects unsafe content using the Content Safety API.\n",
    "\n",
    "        Args:\n",
    "        - media_type (MediaType): The type of media to analyze.\n",
    "        - content (str): The content to analyze.\n",
    "        - blocklists (list[str]): The blocklists to use for text analysis.\n",
    "\n",
    "        Returns:\n",
    "        - dict: The response from the Content Safety API.\n",
    "        \"\"\"\n",
    "        url = self.build_url(media_type)\n",
    "        headers = self.build_headers()\n",
    "        request_body = self.build_request_body(media_type, content, blocklists)\n",
    "        payload = json.dumps(request_body)\n",
    "\n",
    "        response = requests.post(url, headers=headers, data=payload)\n",
    "        print(response.status_code)\n",
    "        print(response.headers)\n",
    "        print(response.text)\n",
    "\n",
    "        res_content = response.json()\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            raise DetectionError(\n",
    "                res_content[\"error\"][\"code\"], res_content[\"error\"][\"message\"]\n",
    "            )\n",
    "\n",
    "        return res_content\n",
    "\n",
    "    def get_detect_result_by_category(\n",
    "        self, category: Category, detect_result: dict\n",
    "    ) -> Union[int, None]:\n",
    "        \"\"\"\n",
    "        Gets the detection result for the given category from the Content Safety API response.\n",
    "\n",
    "        Args:\n",
    "        - category (Category): The category to get the detection result for.\n",
    "        - detect_result (dict): The Content Safety API response.\n",
    "\n",
    "        Returns:\n",
    "        - Union[int, None]: The detection result for the given category, or None if it is not found.\n",
    "        \"\"\"\n",
    "        category_res = detect_result.get(\"categoriesAnalysis\", None)\n",
    "        for res in category_res:\n",
    "            if category.name == res.get(\"category\", None):\n",
    "                return res\n",
    "        raise ValueError(f\"Invalid Category {category}\")\n",
    "\n",
    "    def make_decision(\n",
    "        self,\n",
    "        detection_result: dict,\n",
    "        reject_thresholds: dict[Category, int],\n",
    "    ) -> Decision:\n",
    "        \"\"\"\n",
    "        Makes a decision based on the Content Safety API response and the specified reject thresholds.\n",
    "        Users can customize their decision-making method.\n",
    "\n",
    "        Args:\n",
    "        - detection_result (dict): The Content Safety API response.\n",
    "        - reject_thresholds (dict[Category, int]): The reject thresholds for each category.\n",
    "\n",
    "        Returns:\n",
    "        - Decision: The decision based on the Content Safety API response and the specified reject thresholds.\n",
    "        \"\"\"\n",
    "        action_result = {}\n",
    "        final_action = Action.Accept\n",
    "        for category, threshold in reject_thresholds.items():\n",
    "            if threshold not in (-1, 0, 2, 4, 6):\n",
    "                raise ValueError(\"RejectThreshold can only be in (-1, 0, 2, 4, 6)\")\n",
    "\n",
    "            cate_detect_res = self.get_detect_result_by_category(\n",
    "                category, detection_result\n",
    "            )\n",
    "            if cate_detect_res is None or \"severity\" not in cate_detect_res:\n",
    "                raise ValueError(f\"Can not find detection result for {category}\")\n",
    "\n",
    "            severity = cate_detect_res[\"severity\"]\n",
    "            action = (\n",
    "                Action.Reject\n",
    "                if threshold != -1 and severity >= threshold\n",
    "                else Action.Accept\n",
    "            )\n",
    "            action_result[category] = action\n",
    "            if action.value > final_action.value:\n",
    "                final_action = action\n",
    "\n",
    "        if (\n",
    "            \"blocklistsMatch\" in detection_result\n",
    "            and detection_result[\"blocklistsMatch\"]\n",
    "            and len(detection_result[\"blocklistsMatch\"]) > 0\n",
    "        ):\n",
    "            final_action = Action.Reject\n",
    "\n",
    "        print(final_action.name)\n",
    "        print(action_result)\n",
    "\n",
    "        return Decision(final_action, action_result)\n",
    "\n",
    "\n",
    "\n",
    "def test_moderate_content(content_safety, media_type, content, blocklists):\n",
    "    # Detect content safety\n",
    "    detection_result = content_safety.detect(media_type, content, blocklists)\n",
    "\n",
    "    # Set the reject thresholds for each category\n",
    "    reject_thresholds = {\n",
    "        Category.Hate: 4,\n",
    "        Category.SelfHarm: 4,\n",
    "        Category.Sexual: 4,\n",
    "        Category.Violence: 4,\n",
    "    }\n",
    "\n",
    "    # Make a decision based on the detection result and reject thresholds\n",
    "    decision_result = content_safety.make_decision(detection_result, reject_thresholds)\n",
    "    return decision_result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "{'Transfer-Encoding': 'chunked', 'Content-Type': 'application/json; charset=utf-8', 'csp-billing-usage': 'CognitiveServices.ContentSafety.Text:Analyze=1', 'api-supported-versions': '2023-04-30-preview,2023-05-30-preview,2023-10-01,2023-10-15-preview,2023-10-30-preview,2024-02-15-preview,2024-03-10-preview,2024-03-30-preview,2024-09-01,2024-09-15-preview,2024-09-30-preview', 'x-envoy-upstream-service-time': '51', 'apim-request-id': '6eb67dc6-3a52-4522-b50f-360e979ac976', 'Strict-Transport-Security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'x-ms-region': 'East US', 'Date': 'Wed, 04 Sep 2024 07:08:50 GMT'}\n",
      "{\"blocklistsMatch\":[],\"categoriesAnalysis\":[{\"category\":\"Hate\",\"severity\":0},{\"category\":\"SelfHarm\",\"severity\":0},{\"category\":\"Sexual\",\"severity\":0},{\"category\":\"Violence\",\"severity\":4}]}\n",
      "Reject\n",
      "{<Category.Hate: 1>: <Action.Accept: 1>, <Category.SelfHarm: 2>: <Action.Accept: 1>, <Category.Sexual: 3>: <Action.Accept: 1>, <Category.Violence: 4>: <Action.Reject: 2>}\n",
      "<__main__.Decision object at 0x0000017AA87A4D60>\n"
     ]
    }
   ],
   "source": [
    "# Initialize the ContentSafety object\n",
    "content_safety = ContentSafety(endpoint, subscription_key, api_version)\n",
    "\n",
    "# Set the media type and blocklists\n",
    "media_type = MediaType.Text\n",
    "blocklists = []\n",
    "# Set the content to be tested\n",
    "content = \"The dog was given a eutanasa injection due to their severed leg bleding profusely from deep lacarations to the lower extremities, exposing tisssue and nerve.\"\n",
    "result = test_moderate_content(content_safety, media_type, content, blocklists)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
